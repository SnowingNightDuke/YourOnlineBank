<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Order picking optimization in warehouses and supermarkets with OptaPlanner</title><link rel="alternate" href="https://blog.kie.org/2021/12/order-picking-optimization-in-warehouses-and-supermarkets-with-optaplanner.html" /><author><name>wmedvede</name></author><id>https://blog.kie.org/2021/12/order-picking-optimization-in-warehouses-and-supermarkets-with-optaplanner.html</id><updated>2021-12-02T00:00:00Z</updated><content type="html">  During the pandemic year, we saw many stores and supermarkets adapt their business in several ways. Sometimes these changes occurred from day to day. For example, as a consequence of the sharp decrease in customer physical visits, online orders increased exponentially. How many of them were prepared to deal optimally with this increase… ? In this blog post, we present the new OptaPlanner that shows how to complete online orders in an optimized way! THE ORDER PICKING PROBLEM The order picking problem consists of a set of orders that need to be prepared for delivery to various customers. Each order is composed of a set of order items (the requested products). These products are located on shelves in the warehouse or in the supermarket and occupy a specific volume of space. To complete the orders, there is a set of available trolleys that follow a calculated path in the supermarket and pick the order items. One order item is picked on each step of this path. The location of the products within the warehouse determine the path of the trolley, and the space in each trolley is divided into a number of buckets with a specified capacity. The goal of the order picking problem is to calculate a picking plan that provides the path for each trolley and considers the following constraints: * The distance to travel by the trolleys is minimized. * All the order items must be picked. * Items from different orders mustn’t be mixed in the same bucket, so enough buckets must be reserved for each of the orders that will be prepared on the same trolley. * The bucket’s capacity is not exceeded by the total volume of the items on it. * The splitting of an order into different trolleys should be minimized. The following image shows a simplified view of the order picking problem input data and calculated picking plan: WAREHOUSE STRUCTURE To formulate the problem, the warehouse is defined as a set of shelves organized into columns and rows. Products are located on the left or right side of a specific shelf, on a specific row. The shelf, side, and row determine the product location. THE NEW QUICKSTART PROJECT STRUCTURE The following image shows the maven project that comprises the quickstart: The server side code is in the src/main/java directory and is organized in four packages: bootstrap Helper classes for generating the randomized (but reproducible) data set used by the quickstart on application startup. domain Contains the domain model for representing the problem, and the planning classes used by OptaPlanner, for example the @PlanningSolution and the @PlanningEntity. solver Contains the ConstraintProvider implementation. rest Contains the REST API used by the client-side code to start and stop solving, and to retrieve the best solution to display. The client code is in the src/main/resources/META-INF/resources/ directory. This directory has an index.html file that contains the basic HTML structure and loads the app.js file that calls the server’s REST API and makes the web page dynamic. PLANNING DOMAIN MODEL ORDER PICKING CONSTRAINTS The order picking constraints are implemented using the Constraint Streams API, and they can be found in the org.acme.orderpicking.solver.OrderPickingConstraintProvider class. The following constraints are provided: requiredNumberOfBuckets Hard constraint to enforce the requirement that a trolley has a sufficient number of buckets for holding all of the elements picked along the path for each order, while at the same time the bucket’s capacity is not exceeded, and items from different orders are not mixed in the same bucket. minimizeDistanceFromPreviousTrolleyStep Soft constraint to enforce the requirement that the distance between the current trolley step, and the previous step in the calculated path is minimized. minimizeDistanceFromLastTrolleyStepToPathOrigin Soft constraint to enforce the requirement that the distance between the first trolley stop (the path origin), and the last trolley stop in the calculated path is minimized. The conjunction of the minimizeDistanceFromPreviousTrolleyStep constraint applied to every step on the path, and the minimizeDistanceFromLastTrolleyStepToPathOrigin constraint, enforces the entire path minimization. minimizeOrderSplitByTrolley Soft constraint to minimize the orders splitting into different trolleys. EXECUTION EXPLAINED When you first start the order picker, you will see that no picking plan or paths are calculated. No worries, it’s perfectly fine because the solver isn’t executed yet. Let’s take a moment to take a look at the initial data set before starting the solver. You use the Unassigned tab to do this. UNASSIGNED ENTITIES The information about the unassigned trolleys and orders is shown on the different sub-tabs. Before starting the solver, you can view all of the elements in the initial data set on this tab. However, as soon the solver starts solving, you will see that elements start disappearing from this tab. Good news, they’re becoming part of the picking plan!. Unassigned trolleys Unassigned Order_1 SOLVER EXECUTION Click the Start button to start the solver. When the solver is started, you’ll see that the Picking plan tab will start showing the information about the calculated plan. Note While the solver is running, the calculated plan is refreshed every 2 seconds, causing a screen refresh effect. You can use the stop solving button to mitigate this effect. TROLLEY NAVIGATION Use the Map tab to view a representation of how the different trolleys navigate the warehouse according to the calculated paths. The post appeared first on .</content><dc:creator>wmedvede</dc:creator></entry><entry><title type="html">Implementing the Filter EIP using Camel-k and Kogito</title><link rel="alternate" href="https://blog.kie.org/2021/12/implementing-the-filter-eip-using-camel-k-and-kogito.html" /><author><name>Sadhana Nandakumar</name></author><id>https://blog.kie.org/2021/12/implementing-the-filter-eip-using-camel-k-and-kogito.html</id><updated>2021-12-01T22:30:15Z</updated><content type="html">Apache Camel is an Open Source integration framework that supports the implementation of the Enterprise Integration patterns (EIP). The Message Filter EIP allows you to eliminate undesired messages from a channel based on a set of criteria. The pattern uses a predicate to decide if the messages should be dropped or not.  Oftentimes the predicate logic can be complex, and might need more transparency and control by the business user. For instance, let us say we have transactions coming in, and we are only interested in the high risk transactions. As you can imagine, the rules that govern the risk can be complicated. This is where a rules engine can come in handy, to provide for a business user friendly methodology to define the predicate logic.  In this article we will discuss how to implement the Filter EIP using the cloud native technologies – Kogito and Camel-K. Kogito is designed to deliver powerful capabilities for building process and rules based applications natively in the cloud on a modern container platform. Camel K is a lightweight integration framework built from Apache Camel to run natively on Kubernetes DEFINING THE TRANSACTION RISK RULES Let us first define the transaction risk rules. As you can see below, the transaction risk is based on transactionAmount, transactionCountry and merchant information. DEPLOYING THE DECISION SERVICE The kogito project for this decision can be found . Let us now deploy this DMN as a kogito decision service on openshift. For this install the kogito operator. Now let us create a kogito build using the repo above. After the Build and Deploy the kogito decision service exposes a route so that we can invoke the decision service. We can now invoke the decision service and test it. DEFINING THE CAMEL ROUTE Let us now define the camel route. We first invoke the decision service. The service returns back a payload with a boolean value indicating the risk. We then use this decision result in the predicate for deciding if this transaction needs further processing. As you can see, the complexity of deciding if the transaction is high risk is offloaded to the kogito decision layer. The entire camel route definition can be found . DEPLOYING THE CAMEL ROUTE USING CAMEL-K We will now install the Camel K operator and deploy the integration micro service. kamel install Kamel run FilterEIP.java TESTING THE INTEGRATION MICROSERVICE Now we can lookup the route that is auto generated by the REST DSL component and send in a transaction payload. We can now see that the data is now available in the kafka topic. Notice the header information, where we set the decision evaluation to decide if the transaction needs to be sent to kafka. We will now send in a request, with a low risk. Now, we can see the risk is evaluated to be false and this transaction is filtered for post processing, so we don’t see it on the kafka topic. Similarly, it is possible to plug in decision points to other EIP patterns like content routing. Check out this from Matteo Mortari about how you can use DMN decisions for content based routing in Camel. References The post appeared first on .</content><dc:creator>Sadhana Nandakumar</dc:creator></entry><entry><title>How DevSecOps brings security into the development process</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/01/how-devsecops-brings-security-development-process" /><author><name>Andy Oram</name></author><id>d9d5c475-8943-4b44-a5b9-d478a4eff697</id><updated>2021-12-01T07:00:00Z</updated><published>2021-12-01T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/devsecops"&gt;DevSecOps&lt;/a&gt; is an extension of &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; that emphasizes security automation and cooperation across the organization. More than just hype, DevSecOps is a crucial addition to your organization's development and deployment processes, especially given the range of ransomware groups, industrial spies, identity thieves, and other attackers plaguing today's cyberworld. In this article, you will learn how DevSecOps extends familiar DevOps tools and processes to help cross-functional teams work together on the design and implementation of &lt;a href="https://developers.redhat.com/topics/security"&gt;security&lt;/a&gt; policies and procedures.&lt;/p&gt; &lt;h2&gt;What is DevSecOps?&lt;/h2&gt; &lt;p&gt;Essentially, DevSecOps is a way to ensure that security policies set by your organization—such as static analysis, vulnerability scanning, and access controls—are applied consistently in production, even if you are launching hundreds of virtual machines or containers every hour. With DevSecOps, the tools that carry out security policies are baked into the build process, through well-known DevOps techniques such as &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration and continuous deployment&lt;/a&gt; (CI/CD). The development team can assure managers and administrators that their security policies are being enforced without depending on individual developers to manually run the tools.&lt;/p&gt; &lt;h2&gt;DevSecOps tools and processes&lt;/h2&gt; &lt;p&gt;Developers and teams familiar with DevOps tools and processes can adopt them for DevSecOps. The basic elements of DevSecOps are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: DevSecOps adds vulnerability scanners, penetration testing, firewall rules, intrusion detection systems, and other common security features to the version control and CI/CD processes used for DevOps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Processes&lt;/strong&gt;: DevSecOps automates security practices in order to apply them consistently and verifiably across all the containers and services created by the development team.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparency and review&lt;/strong&gt;: All decisions made by the development team are open to discussion among managers and security experts. Test and production systems can log their activities, and these logs can be checked to ensure that the development team has implemented the decisions of the larger organization.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Let's look at a few hypothetical examples of DevSecOps in action to show how it brings organizational priorities into production.&lt;/p&gt; &lt;h2&gt;Vulnerability scanning in the DevSecOps pipeline&lt;/h2&gt; &lt;p&gt;Vulnerabilities exist at many levels. You may have coding errors such as buffer overflows and incorrect type conversions, poorly secured user interfaces that allow SQL injection, or dependencies on third-party libraries that contain security flaws. A range of automatic vulnerability checkers now exists for all these problems, suited to various application types and sizes. Many of these tools can be added to a build process with just a few clicks in popular developer repositories, such as &lt;a href="https://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/devsecops-in-github"&gt;DevSecOps in GitHub&lt;/a&gt; and &lt;a href="https://about.gitlab.com/solutions/dev-sec-ops/"&gt;DevSecOps with GitLab&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;But a developer can't check everything all the time. The team must decide at what point to run checks, how much time they want to add to the build process, and where the most urgent priorities lie. A DevSecOps pipeline documents these decisions and ensures they are carried out.&lt;/p&gt; &lt;p&gt;For instance, where should you incorporate a tool such as Red Hat's &lt;a href="https://developers.redhat.com/articles/2021/11/25/build-and-extend-containerized-applications-project-thoth"&gt;Project Thoth&lt;/a&gt;, which checks common security databases and reveals flaws in your third-party libraries? If you discover a problem in a function call buried deep within your application, you had better determine right away whether your application is at risk, and if so how to fix the problem. Should you upgrade, back off to an old version of the library, or replace the library altogether? You'll want to learn about the flaw as early in the development cycle as you can, so you might take the time to run the tool upon every check-in to version control.&lt;/p&gt; &lt;p&gt;On the other hand, when it comes to running a common code scanner for, say, bugs in memory management code, you might choose to wait until you are ready to build a full version of the app for testing. Memory management flaws might be critical, but they are usually quick to fix.&lt;/p&gt; &lt;p&gt;The key to DevSecOps is that a team and its security advisors can discuss the tools and trade-offs available for each code base and then bake their decisions into builds.&lt;/p&gt; &lt;h2&gt;Dynamic scanning and penetration testing&lt;/h2&gt; &lt;p&gt;Security experts know that they can't rely on applications to be safe in a production environment, even if the developers have run a battery of static tests. DevSecOps lets developers ensure that every container they launch is checked regularly at runtime by a penetration tester, intrusion detection system, and other such tools.&lt;/p&gt; &lt;h2&gt;An automated approach to securing devices&lt;/h2&gt; &lt;p&gt;Many organizations are losing control over their endpoints. During the COVID-19 shutdowns, workers obtained access to critical systems and assets from their homes and even their local cafes. Within the workplace, "bring your own device" (BYOD) became popular well before that.&lt;/p&gt; &lt;p&gt;The standard security practice in this situation is to scan networks for all devices that connect. A database of approved devices helps restrict access to authorized devices. Such access can also be restricted to particular locations and times of the week. Penetration tests and other tools can even check whether the device has a password and up-to-date software.&lt;/p&gt; &lt;p&gt;To institute such protections, the IT team has to consult with managers of all departments. The organization needs processes for registering devices along with their owners, and for scanning networks regularly. Some of these processes can be incorporated into the development process through DevSecOps. For instance, your configurations can ensure that a scanner is running, as well as a process that monitors logs and alerts.&lt;/p&gt; &lt;p&gt;Similar processes can formalize other organizational protections, such as firewall rules and access control lists.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;DevSecOps is about more than tools and processes. It brings transparency and validation to the crucial area of cybersecurity. All relevant stakeholders can weigh in on security decisions and be sure that the developers incorporate their concerns into development and build processes. Security on a 24/7 basis no longer depends on the day-to-day vigilance of developers or operators; instead, it is enforced by a system where cross-functional teams are working in alignment.&lt;/p&gt; &lt;p&gt;In organizations with constant innovation and heavy dependence on network applications interacting with people around the world, DevSecOps gives developers, operators, and managers more peace of mind.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/01/how-devsecops-brings-security-development-process" title="How DevSecOps brings security into the development process"&gt;How DevSecOps brings security into the development process&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andy Oram</dc:creator><dc:date>2021-12-01T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.5.1.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-5-1-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-5-1-final-released/</id><updated>2021-12-01T00:00:00Z</updated><content type="html">Today, we released Quarkus 2.5.1.Final, a maintenance release for our 2.5 release train containing bugfixes and documentation improvements. It is a safe upgrade for anyone already using 2.5. If you are not using 2.5 already, please refer to the 2.5 migration guide. Full changelog You can get the full changelog...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title type="html">TrustyAI SHAP: Overview and Examples</title><link rel="alternate" href="https://blog.kie.org/2021/11/trustyai-shap-overview-and-examples.html" /><author><name>Rob Geada</name></author><id>https://blog.kie.org/2021/11/trustyai-shap-overview-and-examples.html</id><updated>2021-11-30T09:17:42Z</updated><content type="html">SHAP is soon to be the newest addition to the TrustyAI explanation suite. To properly introduce it, let’s briefly explore what SHAP is, why it’s useful, and go over some tips about how to get the best performance out it. A BRIEF OVERVIEW SHAPLEY VALUES The core idea of a SHAP explanation is that of a Shapley value, a concept from game theory that won its eponymous inventor Lloyd Shapley the 2012 Nobel Prize in Economics. Shapley values are designed to solve the following problem: &gt; Given some coalition C of members that cooperatively produce some value V, how &gt; do we fairly determine the contribution p of each member in C?  Here, a coalition simply refers to a group of cooperating members that work together to produce some value V, called the coalition value. This could be something like corporation of employees that together generate a certain profit, or a dinner group running up a restaurant bill. We want to know exactly how much each member contributed to that final coalition value, what share of the profit each employee deserves, how much each person in the dinner party owes to settle the bill. In simple cases, this can be easy enough to manually figure out, but it can get tricky when there are interacting effects between members, when certain permutations cause members to contribute more or less than the sum of their parts.  A SIMPLE EXAMPLE As an example, let’s look at a coalition that contains 4 members: Red, Orange, Yellow, and Green. Let’s imagine that these are tokens in a game, where your score is calculated as follows: 1. Having the red token adds 5 points to your score. Yellow adds 4 points, while orange and green both add 3. 2. Having any two colors grants a 5 point bonus. 3. Having any three colors grants a 7 point bonus. 4. Having all four colors grants a 10 point bonus. Under these rules, having all four tokens would result in a score of 25: first, we add the raw token values 5+4+3+3=15, then add 10 points from the 4-token bonus. Now the question is, how many of those 25 points were contributed by the red token? This is exactly what Shapley values seek to answer, and do this via a property called average marginal contribution. To compute this, we first look at every possible subcoalition of the members of our coalition, that is, listing all of the different ways of selecting anywhere between 0 and 4 tokens: Then, we organize these coalitions into pairs, such that each member in the pair differ only by the inclusion of the particular member we’re investigating. In this case, we’ll pair coalitions that differ only by the inclusion of the red token: We then find the coalition value of each of these subcombinations, that is, determine the score that each particular coalition of tokens would produce. Then, we find the difference between the two paired coalitions: the difference in score caused by adding the red token: The difference in scores within each pair is called the marginal contribution. Each marginal contribution is then weighted according to how common that particular comparison is. In this example, there are three pairs that compare a 2-token coalition to a 1-token coalition, meaning their weights are each 1/3. Meanwhile, there is only one pair that compares a 1-token coalition to a 0-token coaltion, and thus its weight is 1/1.  We then find the weighted sum of each marginal contribution multiplied by its corresponding weight: Dividing this number by the total number of members in our original coalition (4) gives us the red token’s Shapley value. This is a measure of how much the addition of a red token adds on average to any arbitrary grouping of tokens. In our case, the red token’s Shapley value is 30 ÷ 4 = 7.5, which means that in our original four token hand, the red token contributed 7.5 of our 25 points. We can repeat this process for the other tokens, and now we’ve computed the Shapley values for all four tokens: Therefore, of the 25 points our hand was worth, the red token contributed 7.5 points, the yellow 6.5, and the orange and green 5.5. This seems like very fair way of assigning credit for the total score: each Shapley value is equal to the token’s base score, plus 1/4 of the 10-point four-color bonus.   SHAPLEY VALUES AND EXPLAINABILITY With Shapley values, we now have a means of fairly describing the influence any certain member had on the value of the coalition. We can adapt this concept relatively easily to produce model explanations by instead looking to describe the influence any certain input feature had on the model output. Here, an input feature simply refers to a discrete component of a model’s input; for example, if the model processes loan applications, the input features might be things like the applicant’s name or age. While this seems straightforward enough, computing the Shapley values of an n-feature model in practice requires passing 2n different feature permutations through a model. In situations where we have lots of features or expensive model evaluation, evaluating all 2n combinations may not be feasible. For example, if we had a 25 feature model that takes just one second to evaluate, evaluating all 225 permutations would take over a year! Furthermore, there is an additional practical obstacle: you cannot simply omit a feature from the input of most decision models without having to either retrain or redesign the entire model. SHAP To rectify these problems, Scott Lundberg and Su-In Lee devised the Shapley Kernel in a titled “A Unified Approach to Interpreting Model Predictions”. COMBINATION ENUMERATION To solve the problem of needing to enumerate every possible feature permutation, the Shapley Kernel instead evaluates just a small sample of combinations. Each of the combinations is then assigned some binary vector, where a 1 in the ith position of the vector indicates that the ith feature is included, while 0 indicates exclusion. This gives us a sample of vectors and coalition values, from which we can write a simple system of linear equations. From this system, we can fit a to find A, B, C, and D. With a very specific choice of weightings calculated from various combinametric statistics, we can actually ensure that the weighted linear regression recovers the Shapley values of their respective features! This is the key insight of the Shapley Kernel, that a simple regression over a sample of feature permutations can provide an excellent estimation of Shapley values. With these Shapley values, we can approximate any model with an explanatory model of the form: This gives a very easy to understand, additive explanation of each feature’s contribution to the model output; we can simply look at each Shapley value to know the effect each feature had. BACKGROUND DATA Next to solve is the feature omission problem. Lundberg and Lee do this by simpling reframing what it means to omit a feature. Rather then omit a feature entirely from the model input, they instead replace it with a variety of its typical values, called the background (noise) of the feature. This background feature values are taken from a background dataset, a set of around 100 or so typical inputs to the model. Instead of computing the marginal contribution as the difference between the model output with the feature included versus excluded, we instead find the difference between the model output with the desired feature value versus a background value of that feature. We do this for every background feature value from the background dataset, and take the average of all of those comparisons as our marginal contribution:  Under this reformulation, the Shapley value is measuring how much this particular value of a feature contributed to the model’s output, as compared to background values of the feature.  This introduces a very important concept to SHAP, and that is the background value of the model: the average model output over all the background datapoints. Mathematically, this serves to add an intercept to our explanatory model: Therefore, our model output is equal to the sum of all Shapley values, plus the model background value.  A “REAL WORLD” EXAMPLE Let’s take a look at using SHAP to explain a real world prediction. If you’d like to follow along, the code for this example is found , and the code for generating the plots is . Let’s imagine we’re a bank, and we automatically determine loan eligibility via a . Our model receives seven features as input, the applicant age, their annual income, their number of children, their number of consecutive days of employment, and whether or not they own property, a work phone, or car. From this information, our model outputs a number between 0% and 100%; the applicant’s probability of being accepted for a loan. Let’s say we’ve just received an applicant with the following details: From this input, our model outputs that our applicant has a 65.9% chance of approval. However, if we want to understand why that particular decision was made, we need to be able to provide an explanation of how the applican’ts features affected the output and thus we turn to SHAP. To run SHAP, we need a set of background datapoints from which we can generate our background feature values. In this example, I just randomly generate 100 background datapoints, but when using SHAP for real you might select data points from the model training data or in such a way that the background has desirable properties. In our case, the model output over our 100 randomly-generated background datapoints gave an average approval probability of 46.60%, which thus sets our background value. To better interpret this background, let’s take a look at the average feature value from our background data as compared to our applicant: Here we see that our applicant has a higher-than-background age, income, number of children, and days employed. Like a majority of background dataset applicants, they own realty and do not own a workphone, but are in the minority that own a car. With an idea of how our applicant differs from the typical background applicant, we can now run SHAP to see how these differences affected the model output.  First, we configure SHAP to use these background datapoints, then pass it our random forest classifier and our applicant’s feature data. After SHAP finishes running, we are returned a list of Shapley values, which we then visualize in a candlestick chart: In this chart blue bars indicate that our applicant’s feature difference versus the background has a positive effect on their approval rate, while red bars indicate the difference negatively affected their approval. Therefore, the most positive impact was our applicant’s higher-than-background income, which increased their probability of acceptance by 11.10 percentage points. Meanwhile, their higher-than-typical number of chilren was the most negative impact, reducing their acceptance probability by 1.97 percentage points. Both of these make a lot of intuitive sense; a larger income would naturally increase your ability to pay back a loan, whereas having more children and thus more financial dependents might decrease that ability.  These results indicate that both our random forest model’s decision-making process and our explanatory model’s explanations align with our domain intuition, which is an excellent indicator of the quality of both. This is one of the great advantages of explainability; not only does it provide transparency into model decisions, it also allows for sanity-checking of model behavior.  TIPS AND TRICKS INTERPRETING SHAP EXPLANATIONS As we’ve seen, a SHAP value describes the effect a particular feature had on the model output, as compared to the background features. This comparison can introduce some confusion as to the meaning of the raw Shapley values, and make finding clear intuition a little trickier. This is especially true when the reference points are not clearly defined.  For example: * “Your income increased your acceptance probability by 11.10 percentage points.” * “You own a car, which reduced your acceptance probability 0.67 percentage points.” This explains the income’s effect on the outcome, but does not answer what about the applicant’s income affected the result. Was it too high? Was it too low? There is an explanation, but it is too vague to be actionable in any meaningful way. Instead, we can try directly comparing the applicant’s feature value to the average background value of that feature. This makes the Shapley value’s implicit comparison to the background explicitly clear: * “Your income is $36k higher than a typical income, which increased your acceptance probability by 11.10 percentage points.” * “You own a car, which we see in only 39% of typical loan applicants. Because of this, your acceptance probability was reduced by 0.67 percentage points.” Not only does this convey the comparison being made by the Shapley value, but it also provides the applicant some actionable information about their outcome, e.g.,  “Having a high income and number of consecutive days of employment really helped my loan application, I should take this into account when I’m thinking about retiring.” In a way, this makes a SHAP explanation analagous to a : it informs you of any potential changes you might be able to make to influence your outcome.  MODEL BACKGROUND CHOICE Since the explanations produced by SHAP are comparisons against the background dataset, the choice of background data is highly influential on the returned Shapley values. For example, here are three sets of SHAP explanations for the same model and input (the specific details of which aren’t important), just with three different choices of background: Despite the input data point and model not changing, the directions of the Shapley values vary wildly; for example, the first Shapley value is +13 for the first background selection, +0.27 for the second, and -6.74 with the third. All of these these Shapley values arrive at the same end prediction, and thus are all accurate explanations of the model’s behavior, but convey different information due to their differing comparisons. The choice as to which set of values is better is subjective and domain specific, and as such  the choice of background datapoints needs to done with careful intent. In general, you should try playing around with various choices to see what produces the clearest and most helpful explanations.   CONCLUSIONS In this post, we’ve explored the theoretical foundation of SHAP, examined how the SHAP Kernel Explainer produces explanations, and how we can interpret these explanations as much insight as we can. To see a video overview of SHAP with some different examples, check out SHAP will be joining LIME and Counterfactuals in TrustyAI’s XAI suite shortly, so stay tuned for more updates!    The post appeared first on .</content><dc:creator>Rob Geada</dc:creator></entry><entry><title>Automate dependency analytics with GitHub Actions</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/30/automate-dependency-analytics-github-actions" /><author><name>Divyanshu Agrawal</name></author><id>deb68164-3913-4242-85eb-1588fd2011f1</id><updated>2021-11-30T07:00:00Z</updated><published>2021-11-30T07:00:00Z</published><summary type="html">&lt;p&gt;Security flaws in third-party libraries are so common that many organizations integrate automated dependency analysis into their development workflows. This article introduces a new &lt;a href="https://docs.github.com/en/actions"&gt;GitHub Action&lt;/a&gt; for &lt;a href="https://github.com/redhat-actions/crda"&gt;Red Hat CodeReady Dependency Analytics&lt;/a&gt; that lets you integrate code scanning into your &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration&lt;/a&gt; (CI) pipeline while you are developing your code in a GitHub repository.&lt;/p&gt; &lt;h2&gt;Automating security in the CI/CD pipeline&lt;/h2&gt; &lt;p&gt;Developers always hope that the dependencies that are present or added to a project's manifest are free of vulnerabilities. Over the past few years, software code security has gone from a "nice to have" addition to a necessity. Many organizations today are taking steps to ensure that code going into production does not pose any security risks.&lt;/p&gt; &lt;p&gt;A common solution for this problem is to integrate a code scanning tool into the &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration (CI)&lt;/a&gt; pipeline that checks for vulnerabilities in the incoming dependencies or performs nightly checks to scan for newly introduced security risks. Code scanning lets you detect vulnerabilities prior to release, eliminating the cybersecurity risks that they involve. A number of databases, notably the &lt;a href="https://cve.mitre.org"&gt;CVE list&lt;/a&gt;, provide interfaces to tools that check the databases' contents against dependencies in code.&lt;/p&gt; &lt;p&gt;Integrating a security analysis tool with the CI workflow augments the developers' ability to add security validation while updating incoming dependencies. The addition of the Red Hat CodeReady Dependency Analytics GitHub Action to the GitHub marketplace has made code scanning easier for users in the CI/CD process.&lt;/p&gt; &lt;h2&gt;What is CodeReady Dependency Analytics?&lt;/h2&gt; &lt;p&gt;CodeReady Dependency Analytics, introduced in the article &lt;a href="https://developers.redhat.com/blog/2020/08/28/vulnerability-analysis-with-red-hat-codeready-dependency-analytics-and-snyk"&gt;Vulnerability analysis with Red Hat CodeReady Dependency Analytics and Snyk Intel&lt;/a&gt;, provides vulnerability and compliance analysis for your application's dependencies, along with recommendations to address security vulnerabilities and licensing issues. The &lt;a href="https://snyk.io/product/vulnerability-database/"&gt;Snyk Intel Vulnerability Database&lt;/a&gt; is a curated database of both new and previously known open source software security advisories.&lt;/p&gt; &lt;p&gt;CodeReady Dependency Analytics is also available as a plug-in for &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.fabric8-analytics"&gt;Visual Studio Code&lt;/a&gt;, &lt;a href="https://www.eclipse.org/che/"&gt;Eclipse Che&lt;/a&gt;, &lt;a href="https://developers.redhat.com/products/codeready-workspaces/overview"&gt;Red Hat CodeReady Workspaces,&lt;/a&gt; and &lt;a href="https://www.jetbrains.com/products/"&gt;JetBrains IntelliJ-based IDEs&lt;/a&gt;. These integrations allow developers to "shift left" to detect and fix potential security vulnerabilities in the development process. Using the analytics in an IDE helps fix the security issues early, but adding the same tool into your continuous integration pipeline ensures that you don’t miss any security issues before the code goes into production.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: See &lt;a href="https://developers.redhat.com/blog/2021/04/15/vulnerability-analysis-for-golang-applications-with-red-hat-codeready-dependency-analytics"&gt;Vulnerability analysis for Golang applications with Red Hat CodeReady Dependency Analytics&lt;/a&gt; for an introduction to using CodeReady Dependency Analytics with applications written in Go.&lt;/p&gt; &lt;h2&gt;A GitHub Action for vulnerability analysis&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://github.com/redhat-actions/crda"&gt;CodeReady Dependency Analytics Github Action&lt;/a&gt; is a vulnerability scanner that uses CodeReady Dependency Analytics in the GitHub Actions space. This action scans for vulnerabilities in the project's dependencies and uploads results to the GitHub repository as a &lt;a href="https://github.com/microsoft/sarif-tutorials/blob/main/docs/1-Introduction.md"&gt;SARIF&lt;/a&gt; file, which allows vulnerabilities that are found to be integrated into the project's &lt;strong&gt;Security&lt;/strong&gt; tab. To date, this action supports applications written in Go, Python, Node.js, and Java.&lt;/p&gt; &lt;p&gt;This action also works seamlessly with pull requests, so project owners can check for vulnerabilities in the project dependencies when they are modified by a pull request.&lt;/p&gt; &lt;h2&gt;Setting up the GitHub Action&lt;/h2&gt; &lt;p&gt;The steps described in this article are the minimal steps required to run a CodeReady Dependency Analytics scan on any project. However, you may need to perform additional steps depending on your project requirements.&lt;/p&gt; &lt;h3&gt;Install the command-line interface&lt;/h3&gt; &lt;p&gt;The first thing to configure before running the action is the CodeReady Dependency Analytics command-line interface (CLI). This can be easily installed using the &lt;a href="https://github.com/redhat-actions/openshift-tools-installer"&gt;OpenShift Tools Installer GitHub Action&lt;/a&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- name: Install CRDA cli uses: redhat-actions/openshift-tools-installer@v1 with: source: github crda: latest &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Set up the tool stack&lt;/h3&gt; &lt;p&gt;If not already done, you must set up the tool stack based on your project. For example, if you are running a Java project, you can use the &lt;a href="https://github.com/actions/setup-java"&gt;setup-java action&lt;/a&gt; to set up Java in the workflow.&lt;/p&gt; &lt;h3&gt;Configure authentication&lt;/h3&gt; &lt;p&gt;&lt;a href="https://app.snyk.io/login?utm_campaign=Code-Ready-Analytics-2020&amp;utm_source=code_ready&amp;code_ready=FF1B53D9-57BE-4613-96D7-1D06066C38C9"&gt;Sign up for the Snyk token&lt;/a&gt;, then click through the wizard. You do not need to provide it with any permissions if you don't want to. Go to &lt;strong&gt;Account settings&lt;/strong&gt; to find your Synk token (also known as the &lt;em&gt;key&lt;/em&gt;). Set this token into a &lt;a href="https://docs.github.com/en/actions/reference/encrypted-secrets"&gt;repository secret&lt;/a&gt; and provide the created secret in the &lt;code&gt;synk_token&lt;/code&gt; input.&lt;/p&gt; &lt;p&gt;The workflow looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;name: CRDA scan Java project on: push: pull_request: jobs: scan: name: Analyse Java project runs-on: ubuntu-latest steps: - name: Checkout project uses: actions/checkout@v2 - name: Install CRDA cli uses: redhat-actions/openshift-tools-installer@v1 with: source: github crda: latest - name: Setup Java uses: actions/setup-java@v2 with: distribution: temurin java-version: 11 cache: maven - name: CRDA Scan and upload SARIF id: scan uses: redhat-actions/crda@v1 with: snyk_token: ${{ secrets.SNYK_TOKEN }} - name: Print Report Link run: echo ${{ steps.scan.outputs.report_link }}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the workflow finishes successfully, the &lt;strong&gt;Security&lt;/strong&gt; tab in the GitHub user interface (UI) is populated with the vulnerabilities found and has the relevant details. Figure 1 shows the details of a particular vulnerability.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image1_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image1_1.png?itok=_B9HY-vj" width="1186" height="758" alt="The Security tab in the GitHub interface shows details of vulnerabilities found." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The Security tab in the GitHub interface shows details of vulnerabilities found. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Additional features in the GitHub UI&lt;/h2&gt; &lt;p&gt;Before we finish, let's look at some key features of CodeReady Dependency Analytics that work in the GitHub user interface instead of the CodeReady Dependency Analytics CLI.&lt;/p&gt; &lt;h3&gt;SARIF output and upload&lt;/h3&gt; &lt;p&gt;To support a one-stop solution for dependency scanning and uploading, this GitHub Action supports SARIF output and upload. The action uploads the SARIF file to GitHub, which then displays the vulnerabilities details in the GitHub &lt;strong&gt;Security&lt;/strong&gt; tab as well as inline within pull requests.&lt;/p&gt; &lt;p&gt;Figure 2 shows the vulnerability details in the GitHub UI for a pull request when the code scan finishes.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image2_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image2_1.png?itok=s4v_jRfo" width="1123" height="797" alt=" Each pull request is enhanced with detailed notifications of vulnerabilities found." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. Each pull request is enhanced with detailed notifications of vulnerabilities found. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Code scanning in pull requests&lt;/h3&gt; &lt;p&gt;Many organizations devote time to code reviews, and having another set of eyes on the modified code is always good. Organizations have also started to realize the value of reviewing security issues in the code, but not everyone has the expertise to look for them. Therefore, adding code scanning to the pull request checks substantially aids the reviews of pull requests.&lt;/p&gt; &lt;p&gt;The CodeReady Dependency Analytics GitHub Action can run code scans on pull requests. To scan pull requests, the pull request's code must be checked out. Therefore, to avoid running malicious code, the action comes with a preconfigured mechanism for labeling pull requests that allows maintainers to verify a pull request's code before installing any dependencies or running scans. Once the CodeReady Dependency Analytics scan is complete, the pull request is labeled according to the scan’s result, as shown in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image3_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image3_0.png?itok=Gw7gD-HL" width="941" height="514" alt="A scan that runs on each pull request adds a label to it." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. A scan that runs on each pull request adds a label to it. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Manifest autodetection&lt;/h3&gt; &lt;p&gt;The CodeReady Dependency Analytics GitHub Action makes it easier to scan a project’s dependencies by providing a built-in manifest detection feature. If the project uses a standard default manifest name, the user doesn’t have to explicitly provide it. However, the manifest name can be provided in the input &lt;code&gt;manifest_file&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Dependency installation&lt;/h3&gt; &lt;p&gt;The CodeReady Dependency Analytics CLI expects users to install project dependencies before running a code scan. This CodeReady Dependency Analytics GitHub Action makes dependency installation easier by automatically installing dependencies. The action has a default installation command corresponding to each supported language, which generally fulfills most projects' requirements. However, you can provide a custom dependency installation command using &lt;code&gt;deps_install_cmd&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Millions of people use GitHub for software development, and most incorporate dependencies from open source libraries. The only way to keep up with newly discovered vulnerabilities and avoid inserting these security risks into your application is to integrate scanning into your routine workflows. The CodeReady Dependency Analytics GitHub Action provides invaluable protection to your code by finding and displaying vulnerabilities while you are developing your code on GitHub.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/30/automate-dependency-analytics-github-actions" title="Automate dependency analytics with GitHub Actions"&gt;Automate dependency analytics with GitHub Actions&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Divyanshu Agrawal</dc:creator><dc:date>2021-11-30T07:00:00Z</dc:date></entry><entry><title type="html">Edge Medical Diagnosis - Example architecture with GitOps</title><link rel="alternate" href="http://www.schabell.org/2021/11/edge-medical-diagnosis-with-gitops.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/11/edge-medical-diagnosis-with-gitops.html</id><updated>2021-11-30T06:00:00Z</updated><content type="html">Part 4 - Example architecture with GitOps In our  from this series we talked about the example predictive analysis architecture found in an edge medical diagnosis solution for the healthcare industry. The process was laid out how we approached the use case and how portfolio solutions are the base for researching a generic architecture. It continued by discussion how we approached the use case by researching successful customer portfolio solutions as the basis for a generic architecture. Now it's time to look at one final example architecture. This article walks you through an example architecture for using GitOps for providing a deployment and development example for edge medical diagnosis scenarios. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are based on real solutions using open source technologies. The example scenario presented here is a generic common architecture that was uncovered while researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this architecture and outline the solution for two views of the edge medical diagnosis architecture solution. EDGE MEDICAL DIAGNOSIS WITH GITOPS After touring the solution architecture and elements in the previous article related to actual edge medical diagnosis, this diagram shows us something entirely different. It's a matter of perspective and now we are looking at it from a development and operations perspective, using the lens of GitOps. In the diagram above we start on the right side by the developer and IT operations users, who both are developing their own specific code bases. The developer is working on services, applications, and other integration aspects in their projects. Over in IT operations they are concerned with deployments and infrastructure as code for both agility and scalability, using for example configuration as code projects. Both of these users are pushing their contributions into the source code management (SCM) system where it's then pulled into the CI/CD pipeline for the build processing which could require the use of, for example, standard base images from the remote image repository (shown here as the Red Hat repository). These builds result in application and service images in the image repository for developers which is then pushed out to the subscribed diagnostic facilities in the field. For the IT operations you'll see their configuration and manifest code pushed out to the subscribed diagnostic facilities where they host source code management (SCM) systems in the field.  At this point we transition our discussion to the diagnostic facility where the field application of the medical diagnosis solutions are applied. A GitOps element can be found that uses Argo CD to manage specific resources, including images and operations code based configurations and manifests to define the management of users, deployments, and architectural behaviours.  Finally, we see this all come to fruition on the far left of this diagram as the control plane, a container platform based on OpenShift, hosts the various services and applications in their container-based runtimes. Next up, a look at the architectural solution with a focus on the data view. EDGE MEDICAL DIAGNOSIS WITH GITOPS (DATA) Data connectivity through the edge medical diagnosis architecture with GitOps provides a different look at the architecture and gives us insights into how one of the most valuable assets of a healthcare organisation is being processed. It should be seen as the architecture is intended, as a guide and not a definitive must-do-it-this-way statement on how the data is being routed through as actors are engaging with the systems, applications, and services in this architecture. Note that many of the data flows only one direction while it's fairly obvious it's going to flow both ways. We've chosen to note that in the flows that do not disrupt the clarity of the architecture diagram, and chose not to indicate where the intent is to show processing and flows for clarity from actors to systems on the backend. It's left to the reader to explore these data diagrams and feel free to send comments our way. WHAT'S NEXT This was an overview of the specific pneumonia detection schematic diagrams that make up our architecture for the edge medical diagnosis use case.  An overview of this series on edge medical diagnosis architecture: 1. 2. 3. 4. Catch up on any articles you missed by following one of the links above. This completes the series and we hope you enjoyed this edge medical diagnosis architectural tour.</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">Inspecting the Quarkus Native call path universe with Neo4j</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-native-neo4j-call-tree/" /><author><name>Galder Zamarreño</name></author><id>https://quarkus.io/blog/quarkus-native-neo4j-call-tree/</id><updated>2021-11-30T00:00:00Z</updated><content type="html">This blog post is the culmination of an idea that Sanne (Grinovero) floated to me during some lunch, back at a time when we, remote engineers, would occasionally meet face to face and have the opportunity to share ideas spontaneously. I’m unsure if the lunch was in Neuchâtel or Barcelona,...</content><dc:creator>Galder Zamarreño</dc:creator></entry><entry><title type="html">Upcoming news from Java 18</title><link rel="alternate" href="http://www.mastertheboss.com/java/upcoming-news-from-java-18/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=upcoming-news-from-java-18" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/upcoming-news-from-java-18/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=upcoming-news-from-java-18</id><updated>2021-11-29T11:09:12Z</updated><content type="html">Java 18 is scheduled to be released in the first quarter of 2022. However, there’s already a list of JDK Enhancement Proposals (JEPs). In this article, we will have an overview of the most interesting ones. UTF-8 by Default – JEP 400 How to know Java’s default Character Set ? the obvious answer is via: ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Red Hat Developer roundup: Best of November 2021</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/29/red-hat-developer-roundup-best-november-2021" /><author><name>Red Hat Developer Editorial Team</name></author><id>fb3f2ca8-0f6b-4f31-8bab-659c75d901da</id><updated>2021-11-29T07:00:00Z</updated><published>2021-11-29T07:00:00Z</published><summary type="html">&lt;p&gt;November brought a full plate of new features written by developers working at Red Hat, and we are here to share the harvest. Keep reading for some of our reader's top picks and a few we really like, including updates for &lt;a href="https://developers.redhat.com/articles/2021/11/10/rhel-85-openjdk-17-net-6-and-more"&gt;Red Hat Enterprise Linux 8.5&lt;/a&gt; and &lt;a href="https://developers.redhat.com/articles/2021/11/03/red-hat-enterprise-linux-9-beta-here"&gt;Red Hat Enterprise Linux 9 Beta&lt;/a&gt; and developer guides to &lt;a href="https://developers.redhat.com/articles/2021/11/02/how-choose-best-java-garbage-collector"&gt;choosing your best Java garbage collector&lt;/a&gt;, &lt;a href="https://developers.redhat.com/articles/2021/11/16/building-machine-learning-models-cloud"&gt;building machine learning models in the cloud&lt;/a&gt;, &lt;a href="https://developers.redhat.com/articles/2021/11/22/bring-your-kubernetes-workloads-edge"&gt;bringing Kubernetes workloads to the edge&lt;/a&gt;, and so much more.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: See the end of this article for the full lineup published in November 2021.&lt;/p&gt; &lt;h2&gt;How to get RHEL 8.5 and RHEL 9 Beta&lt;/h2&gt; &lt;p&gt;The best news for many developers this month was the arrival of Red Hat Enterprise Linux 8.5 and Red Hat Enterprise Linux 9 Beta. Find out &lt;a href="https://developers.redhat.com/articles/2021/11/10/rhel-85-openjdk-17-net-6-and-more"&gt;what's new in RHEL 8.5&lt;/a&gt;—including support for OpenJDK 17 and .NET 6—then look ahead to &lt;a href="https://developers.redhat.com/articles/2021/11/03/red-hat-enterprise-linux-9-beta-here"&gt;new features in RHEL 9 Beta&lt;/a&gt;. Both versions are available as a &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;no-cost RHEL for developers subscription&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;See these related articles for more about what you can do with RHEL 8.5:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/15/net-60-now-available-rhel-and-openshift"&gt;.NET 6 now available for RHEL and OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/05/improve-udp-performance-rhel-85"&gt;Improve UDP performance in RHEL 8.5&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Machine learning and data science in the cloud&lt;/h2&gt; &lt;p&gt;As of this month, our collection of resources for building intelligent applications includes new ways to try out &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science"&gt;Red Hat OpenShift Data Science&lt;/a&gt;. Get an overview of this cloud-based &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;AI/ML&lt;/a&gt; platform with Audrey Reznik's &lt;a href="https://developers.redhat.com/articles/2021/11/16/building-machine-learning-models-cloud"&gt;Building machine learning models in the cloud&lt;/a&gt;, then watch these video demonstrations from the OpenShift Data Science team:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/22/access-more-data-your-jupyter-notebook"&gt;Access more data from your Jupyter notebook&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/22/build-and-deploy-object-detection-model-using-openshift-data-science"&gt;Build and deploy an object detection model using OpenShift Data Science&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you like machine learning, you might also enjoy this inside look at &lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;Project Thoth's cloud-based dependency resolver&lt;/a&gt;, which uses reinforcement learning and custom criteria to resolve Python library dependencies.&lt;/p&gt; &lt;h2&gt;Java tools and performance&lt;/h2&gt; &lt;p&gt;It's been a great month for diving into Java performance and tools for profiling and monitoring in the JVM. If you're new to these topics, we recommend starting with Aashish Patil's &lt;a href="https://developers.redhat.com/articles/2021/11/02/how-choose-best-java-garbage-collector"&gt;How to choose the best Java garbage collector&lt;/a&gt;. Then, see:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/18/runtime-profiling-openjdks-hotspot-jvm"&gt;Runtime profiling in OpenJDK's HotSpot JVM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/16/jvm-performance-monitoring-jmc-agent"&gt;JVM performance monitoring with JMC agent&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/23/faster-way-access-jdk-flight-recorder-data"&gt;A faster way to access JDK Flight Recorder data&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Developer Sandbox for Red Hat OpenShift&lt;/h2&gt; &lt;p&gt;As a developer, you want to learn, but maybe your shop isn't working with state-of-the-art technologies. No problem! You can get hands-on experience with Kubernetes, microservices, machine learning, and more with the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, free with a Red Hat account. &lt;a href="https://developers.redhat.com/developer-sandbox/activities/get-started-with-your-developer-sandbox"&gt;Get started with your Developer Sandbox&lt;/a&gt; today, then check out the &lt;a href="https://developers.redhat.com/developer-sandbox/activities"&gt;growing menu of activities&lt;/a&gt; that you can try.&lt;/p&gt; &lt;h2&gt;Debugging, authentication, microservices, and more&lt;/h2&gt; &lt;p&gt;Before you go, here are five more articles we (and our readers) think you might enjoy:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/22/bring-your-kubernetes-workloads-edge"&gt;Bring your Kubernetes workloads to the edge&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/23/how-design-state-machines-microservices"&gt;How to design state machines for microservices&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/09/biometric-authentication-webauthn-and-sso"&gt;Biometric authentication with WebAuthn and SSO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/01/debug-memory-errors-valgrind-and-gdb"&gt;Debug memory errors with Valgrind and GDB&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/11/simplify-kafka-authentication-nodejs"&gt;Simplify Kafka authentication with Node.js&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;November 2021 on Red Hat Developer&lt;/h2&gt; &lt;p&gt;Here are all the articles published so far in November:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/25/build-and-extend-containerized-applications-project-thoth"&gt;Build and extend containerized applications with Project Thoth&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/25/how-we-implemented-authorization-cache-envoy-proxy"&gt;How we implemented an authorization cache for Envoy proxy&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/24/change-deployments-fly-openshift-48"&gt;Change deployments on the fly in OpenShift 4.8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/24/normalize-web-services-camel-k-and-atlasmap-part-1"&gt;Normalize web services with Camel K and AtlasMap, Part 1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/23/how-design-state-machines-microservices"&gt;How to design state machines for microservices&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/23/faster-way-access-jdk-flight-recorder-data"&gt;A faster way to access JDK Flight Recorder data&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/22/bring-your-kubernetes-workloads-edge"&gt;Bring your Kubernetes workloads to the edge&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/22/access-more-data-your-jupyter-notebook"&gt;Access more data from your Jupyter notebook&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/22/build-and-deploy-object-detection-model-using-openshift-data-science"&gt;Build and deploy an object detection model using OpenShift Data Science&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/19/improve-multicore-scaling-open-vswitch-dpdk"&gt;Improve multicore scaling in Open vSwitch DPDK&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/18/runtime-profiling-openjdks-hotspot-jvm"&gt;Runtime profiling in OpenJDK's HotSpot JVM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/18/build-and-store-universal-application-images-openshift"&gt;Build and store universal application images on OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/18/design-authorization-cache-envoy-proxy-using-webassembly"&gt;Design an authorization cache for Envoy proxy using WebAssembly&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/17/managing-persistent-volume-access-kubernetes"&gt;Managing persistent volume access in Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;Customize Python dependency resolution with machine learning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/16/building-machine-learning-models-cloud"&gt;Building machine learning models in the cloud&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/16/jvm-performance-monitoring-jmc-agent"&gt;JVM performance monitoring with JMC agent&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/16/custom-jfr-event-templates-cryostat-20"&gt;Custom JFR event templates with Cryostat 2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/15/net-60-now-available-rhel-and-openshift"&gt;.NET 6 now available for RHEL and OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/15/red-hat-software-collections-38-and-red-hat-developer-toolset-11-now-generally"&gt;Red Hat Software Collections 3.8 and Red Hat Developer Toolset 11 now generally available&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/11/best-practices-building-images-pass-red-hat-container-certification"&gt;Best practices for building images that pass Red Hat Container Certification&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/11/simplify-kafka-authentication-nodejs"&gt;Simplify Kafka authentication with Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/10/rhel-85-openjdk-17-net-6-and-more"&gt;RHEL 8.5: OpenJDK 17, .NET 6, and more&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/09/biometric-authentication-webauthn-and-sso"&gt;Biometric authentication with WebAuthn and SSO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/10/kafka-monthly-digest-october-2021"&gt;Kafka Monthly Digest: October 2021&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/09/automating-jdk-flight-recorder-containers"&gt;Automating JDK Flight Recorder in containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/08/test-driven-development-quarkus"&gt;Test-driven development with Quarkus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/08/optimize-nodejs-images-ubi-8-nodejs-minimal-image"&gt;Optimize Node.js images with the UBI 8 Node.js minimal image&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/05/improve-udp-performance-rhel-85"&gt;Improve UDP performance in RHEL 8.5&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/04/knative-10-simplify-serverless-kubernetes"&gt;Knative 1.0: Simplify serverless on Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/04/boost-throughput-resteasy-reactive-quarkus-22"&gt;Boost throughput with RESTEasy Reactive in Quarkus 2.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/04/generating-pseudorandom-numbers-python"&gt;Generating pseudorandom numbers in Python&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/03/red-hat-enterprise-linux-9-beta-here"&gt;Red Hat Enterprise Linux 9 Beta is here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/03/best-nodeconf-remote-30-second-review"&gt;Best of NodeConf Remote: The 30-second review&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/03/how-use-service-binding-rabbitmq"&gt;How to use service binding with RabbitMQ&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/02/how-choose-best-java-garbage-collector"&gt;How to choose the best Java garbage collector&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/02/java-monitoring-custom-targets-cryostat"&gt;Java monitoring for custom targets with Cryostat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/01/debug-memory-errors-valgrind-and-gdb"&gt;Debug memory errors with Valgrind and GDB&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/29/red-hat-developer-roundup-best-november-2021" title="Red Hat Developer roundup: Best of November 2021"&gt;Red Hat Developer roundup: Best of November 2021&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Red Hat Developer Editorial Team</dc:creator><dc:date>2021-11-29T07:00:00Z</dc:date></entry></feed>
